{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents<a id='1.1_Contents'></a>\n",
    "* [1. Data Wrangling](#1_Data_wrangling)\n",
    "* [2. Exploraratory Data Analysis](#2_Exploratory_Data_Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data wrangling<a id='2_Data_wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents<a id='1.1_Contents'></a>\n",
    "* [1.1 Introduction](#1_Introduction)\n",
    "* [1.2 Imports](#1.1_Imports)\n",
    "* [1.3 Objectives](#1.2_Objectives)\n",
    "* [1.4 Load Data](#1.3_Load_Data)\n",
    "* [1.5 Explore Data](#1.4_Explore_Data)\n",
    "* [1.5.1 Number Of Missing Values By Column](#1.4.1_Number_Of_Missing_Values_By_Column)\n",
    "* [1.5.2 Numeric Features](#1.4.2_Numeric_Features)\n",
    "  * [1.5.2.1 Numeric data summary](#1.4.2.1_Numeric_data_summary)\n",
    "  * [1.5.2.2 Distributions Of Feature Values](#1.4.2.2_Distributions_Of_Feature_Values)\n",
    "    * [1.5.2.2.1 Solids](#1.4.2.2.1_Solids)\n",
    "* [1.6 Derive State-wide Summary Statistics](#1.5_Derive_State-wide_Summary_Statistics)\n",
    "* [1.7 Save Data](#1.6_Save_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction<a id='1_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step focuses on collecting your data, organizing it, and making sure it's well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Quality\n",
    "\n",
    "https://www.kaggle.com/adityakadiwal/water-potability/activity\n",
    "\n",
    "Context\n",
    "Access to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions.\n",
    "\n",
    "Content\n",
    "The water_potability.csv file contains water quality metrics for 3276 different water bodies.\n",
    "\n",
    "**1. pH value:**\n",
    "PH is an important parameter in evaluating the acid–base balance of water. It is also the indicator of acidic or alkaline condition of water status. WHO has recommended maximum permissible limit of pH from 6.5 to 8.5. The current investigation ranges were 6.52–6.83 which are in the range of WHO standards.\n",
    "\n",
    "**2. Hardness:**\n",
    "Hardness is mainly caused by calcium and magnesium salts. These salts are dissolved from geologic deposits through which water travels. The length of time water is in contact with hardness producing material helps determine how much hardness there is in raw water. Hardness was originally defined as the capacity of water to precipitate soap caused by Calcium and Magnesium.\n",
    "\n",
    "**3. Solids (Total dissolved solids - TDS):**\n",
    "Water has the ability to dissolve a wide range of inorganic and some organic minerals or salts such as potassium, calcium, sodium, bicarbonates, chlorides, magnesium, sulfates etc. These minerals produced un-wanted taste and diluted color in appearance of water. This is the important parameter for the use of water. The water with high TDS value indicates that water is highly mineralized. Desirable limit for TDS is 500 mg/l and maximum limit is 1000 mg/l which prescribed for drinking purpose.\n",
    "\n",
    "**4. Chloramines:**\n",
    "Chlorine and chloramine are the major disinfectants used in public water systems. Chloramines are most commonly formed when ammonia is added to chlorine to treat drinking water. Chlorine levels up to 4 milligrams per liter (mg/L or 4 parts per million (ppm)) are considered safe in drinking water.\n",
    "\n",
    "**5. Sulfate:**\n",
    "Sulfates are naturally occurring substances that are found in minerals, soil, and rocks. They are present in ambient air, groundwater, plants, and food. The principal commercial use of sulfate is in the chemical industry. Sulfate concentration in seawater is about 2,700 milligrams per liter (mg/L). It ranges from 3 to 30 mg/L in most freshwater supplies, although much higher concentrations (1000 mg/L) are found in some geographic locations.\n",
    "\n",
    "**6. Conductivity:**\n",
    "Pure water is not a good conductor of electric current rather’s a good insulator. Increase in ions concentration enhances the electrical conductivity of water. Generally, the amount of dissolved solids in water determines the electrical conductivity. Electrical conductivity (EC) actually measures the ionic process of a solution that enables it to transmit current. According to WHO standards, EC value should not exceeded 400 μS/cm.\n",
    "\n",
    "**7. Organic_carbon:**\n",
    "Total Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. TOC is a measure of the total amount of carbon in organic compounds in pure water. According to US EPA < 2 mg/L as TOC in treated / drinking water, and < 4 mg/Lit in source water which is use for treatment.\n",
    "\n",
    "**8. Trihalomethanes:**\n",
    "THMs are chemicals which may be found in water treated with chlorine. The concentration of THMs in drinking water varies according to the level of organic material in the water, the amount of chlorine required to treat the water, and the temperature of the water that is being treated. THM levels up to 80 ppm is considered safe in drinking water.\n",
    "\n",
    "**9. Turbidity:**\n",
    "The turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of light emitting properties of water and the test is used to indicate the quality of waste discharge with respect to colloidal matter. The mean turbidity value obtained for Wondo Genet Campus (0.98 NTU) is lower than the WHO recommended value of 5.00 NTU.\n",
    "\n",
    "**10. Potability:**\n",
    "Indicates if water is safe for human consumption where 1 means Potable and 0 means Not potable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports<a id='1.1_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:57.349128Z",
     "start_time": "2021-07-17T14:19:55.202703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default libraries: pandas, numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "# import urbangrammar_graphics as ugg\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# Load collections of functions\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from clustergram import Clustergram\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_validate, learning_curve, GridSearchCV, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# # insert the \"import_test\" directory into the sys.path\n",
    "# sys.path\n",
    "# sys.path.insert(1, os.path.abspath(\"..\"))\n",
    "\n",
    "# from lib.multiply import multiplier\n",
    "# from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Objectives<a id='1.2_Objectives'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the data seems okay for the questions to be answered:\n",
    "    - Define the target variable.\n",
    "    - Useful features.\n",
    "- Fundamental issues with the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load Data<a id='1.3_Load_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.083136Z",
     "start_time": "2021-07-17T14:19:57.352120Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\map_f\\\\OneDrive\\\\Documents\\\\Dropbox\\\\Springboard\\\\GitHub\\\\Springboard\\\\Classification\\\\Water_potability\\\\water_potability.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-897a8f977182>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\Water_potability\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwater\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'water_potability.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# credit_record = pd.read_csv(path + 'credit_record.csv',index_col= None,delimiter=',')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# type(raw_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\map_f\\\\OneDrive\\\\Documents\\\\Dropbox\\\\Springboard\\\\GitHub\\\\Springboard\\\\Classification\\\\Water_potability\\\\water_potability.csv'"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + '\\\\Water_potability\\\\'\n",
    "\n",
    "water = pd.read_csv(path + 'water_potability.csv',index_col= None,delimiter=',')\n",
    "# credit_record = pd.read_csv(path + 'credit_record.csv',index_col= None,delimiter=',')\n",
    "# type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.099063Z",
     "start_time": "2021-07-17T14:19:55.211Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 2#\n",
    "#Call the info method on ski_data to see a summary of the data\n",
    "water.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Potability` indicates if water is safe for human consumption. 1 means Potable and 0 means Not Potable. The other columns are features explained above.\n",
    "- Numerical features: float64(9), int64(1)\n",
    "- Categorical: none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.101057Z",
     "start_time": "2021-07-17T14:19:55.218Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "water.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Headers: related to the problem.\n",
    "- Missing values: present - further investigation required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Explore Data<a id='1.4_Explore_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Number Of Missing Values By Column<a id='1.4.1_Number_Of_Missing_Values_By_Column'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of missing values in each column and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.102054Z",
     "start_time": "2021-07-17T14:19:55.226Z"
    }
   },
   "outputs": [],
   "source": [
    "#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of data\n",
    "#as well as the percentages (using `.mean()` instead of `.sum()`).\n",
    "#Order them (increasing or decreasing) using sort_values\n",
    "missing = pd.concat([water.isnull().sum(), 100 * water.isnull().mean()], axis=1)\n",
    "missing.columns=['count','%']\n",
    "missing = missing[missing['count'] > 0]\n",
    "missing.sort_values(by='%',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Showing only column with missing values\n",
    "- Columns with missing data: 3\n",
    "- Most missing `Sufalte`: 23%\n",
    "- Target missing: none\n",
    "- Notes:\n",
    "    - Check for non genunine values (-1, 999)\n",
    "    - Check for irrational values\n",
    "    - Check outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Numeric Features<a id='1.4.2_Numeric_Features'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having decided to reserve judgement on how exactly you utilize the State, turn your attention to cleaning the numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2.1 Numeric data summary<a id='1.4.2.1_Numeric_data_summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.104051Z",
     "start_time": "2021-07-17T14:19:55.234Z"
    }
   },
   "outputs": [],
   "source": [
    "#Call data's `describe` method for a statistical summary of the numerical columns\n",
    "water.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2.2 Distributions Of Feature Values<a id='1.4.2.2_Distributions_Of_Feature_Values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.106044Z",
     "start_time": "2021-07-17T14:19:55.238Z"
    }
   },
   "outputs": [],
   "source": [
    "#Call data's `hist` method to plot histograms of each of the numeric features\n",
    "water.hist(figsize=(15,10))\n",
    "plt.subplots_adjust(hspace=0.5);\n",
    "#Hint: notice how the terminating ';' \"swallows\" some messy output and leads to a tidier notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plausible or wrong: all features have a distribution similar to the normal\n",
    "- Outliers: maybe solids can have some outliers, but there are no obvious outlieres in general\n",
    "- Constant values: none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2.2.1 Solids<a id='1.4.2.2.1_Solids'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.108039Z",
     "start_time": "2021-07-17T14:19:55.243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "water.Solids[water.Solids > 50000].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The highest values for `Solids` are reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Derive Summary Statistics<a id='1.5_Derive_State-wide_Summary_Statistics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.109035Z",
     "start_time": "2021-07-17T14:19:55.254Z"
    }
   },
   "outputs": [],
   "source": [
    "#Add named aggregations for the sum of the variables and reset_index()\n",
    "water_summary = water.groupby('ph').agg(\n",
    "#     count=pd.NamedAgg(column='Hardness', aggfunc='size'), #could pick any column here\n",
    "    Hardness=pd.NamedAgg(column='Hardness', aggfunc='sum'),\n",
    "    Solids=pd.NamedAgg(column='Solids', aggfunc='sum'),\n",
    "    Chloramines=pd.NamedAgg(column='Chloramines', aggfunc='sum'),\n",
    "    Sulfate=pd.NamedAgg(column='Sulfate', aggfunc='sum'),\n",
    "    Conductivity=pd.NamedAgg(column='Conductivity', aggfunc='sum'),\n",
    "    Organic_carbon=pd.NamedAgg(column='Organic_carbon', aggfunc='sum'),\n",
    "    Trihalomethanes=pd.NamedAgg(column='Trihalomethanes', aggfunc='sum'),\n",
    "    Turbidity=pd.NamedAgg(column='Turbidity', aggfunc='sum'),\n",
    "# Potability=pd.NamedAgg(column='Potability', aggfunc='sum'),\n",
    ").reset_index()\n",
    "water_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.111031Z",
     "start_time": "2021-07-17T14:19:55.260Z"
    }
   },
   "outputs": [],
   "source": [
    "water.groupby('ph').sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Exploratory Data Analysis<a id='2_Exploratory_Data_Analysis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents<a id='2.1_Contents'></a>\n",
    "  * [2.1 Principle components analysis](#2.1_PCA)\n",
    "    * [2.1.1 Scale the data](#2.1.1_Scale_the_data)\n",
    "      * [2.1.1.1 Verifying the scaling](#2.1.1.1_Verifying_the_scaling)\n",
    "  * [2.2 Calculate the PCA transformation](#2.2_PCA_transformation)\n",
    "  * [2.3 Average potability by ph](#2.3_Average_target_by_descriptor)\n",
    "  * [2.4 Adding average potability to scatter plot](#2.4_Adding_average_target_to_scatter_plot)\n",
    "  * [2.5 Conclusion On How To Handle ph](#2.5_Conclusion_On_How_To_descriptor)\n",
    "    * [2.5.1 Numeric Data](#2.5.1_Resort_Numeric_Data)\n",
    "      * [2.5.1.1 Feature engineering](#2.5.1.1_Feature_engineering)\n",
    "      * [2.5.1.2 Feature correlation heatmap](#2.5.1.2_Feature_correlation_heatmap)\n",
    "      * [2.5.1.3 Scatterplots of numeric features Potability](#2.5.1.3_Scatterplots_of_numeric_features_against_target)\n",
    "  * [2.6 Summary](#2.6_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the common knowledge about water potability, let's evaluate the values that we have in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.113026Z",
     "start_time": "2021-07-17T14:19:55.267Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = water\n",
    "# bins = [-np.inf, 0, 6.5, 8.5, np.inf],\n",
    "# names=['out_range_low', 'acid', 'potable','basic','out_range_high']\n",
    "# column = 'ph'\n",
    "# def categorize(df, column, bins, names):\n",
    "# #     a = df[columns]\n",
    "# #     df[column + '_cat'] = pd.cut(df, bins , labels=names, include_lowest=True)\n",
    "# #     df\n",
    "# #     df = df.join(c.add_suffix('_cat'))\n",
    "# #     c = pd.cut(df.stack(), bins, labels = names)\n",
    "# #     df_cat = df.join(c.unstack().add_suffix('_cat'))\n",
    "# #     ranges = df_cat.groupby(['Potability',column+'_cat']).agg(['count','min','max'])\n",
    "# #     ranges[column]\n",
    "# categorize(df,'ph',bins,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.115020Z",
     "start_time": "2021-07-17T14:19:55.273Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1. pH\n",
    "# bins = [-np.inf, 0, 6.5, 8.5, 14, np.inf]\n",
    "# labels = ['out_range_low', 'acid', 'potable','basic','out_range_high']\n",
    "# categorize(water,'ph',bins,labels)\n",
    "\n",
    "# # 2. Hardness\n",
    "# # bins = [-np.inf, 0, 6.5, 8.5, 14, np.inf]\n",
    "# # names = ['out_range_low', 'acid', 'potable','basic','out_range_high']\n",
    "# # categorize(water,'Potability',bins,labels)\n",
    "\n",
    "# # 3. Solids\n",
    "# bins = [-np.inf, 0, 500, 1000, np.inf],\n",
    "# labels=['out_range_low', 'low', 'potable','high']\n",
    "# categorize(water,'Solids',bins,labels)\n",
    "\n",
    "# # 4. Chloramines\n",
    "# bins = [-np.inf, 0, 4, np.inf],\n",
    "# labels=['out_range_low', 'potable','high']\n",
    "# categorize(water,'Chloramines',bins,labels)\n",
    "\n",
    "# # 5. Sulfate\n",
    "# bins = [-np.inf, 0, 3, 30, np.inf],\n",
    "# labels=['out_range_low', 'low', 'potable','high']\n",
    "# categorize(water,'Sulfate',bins,labels)\n",
    "\n",
    "# # 6. Conductivity\n",
    "# bins = [-np.inf, 0, 400, np.inf],\n",
    "# labels=['out_range_low', 'potable','high']\n",
    "# categorize(water,'Conductivity',bins,labels)\n",
    "\n",
    "# # 7. Organic_carbon\n",
    "# bins = [-np.inf, 0, 2, 4, np.inf],\n",
    "# labels=['out_range_low', 'potable','to be treated','high']\n",
    "# categorize(water,'Organic_carbon',bins,labels)\n",
    "\n",
    "# # 8. Trihalomethanes\n",
    "# bins = [-np.inf, 0, 80, np.inf],\n",
    "# labels=['out_range_low', 'potable','high']\n",
    "# categorize(water,'Trihalomethanes',bins,labels)\n",
    "\n",
    "# # 9. Turbidity\n",
    "# bins =  [-np.inf, 0, 5, np.inf],\n",
    "# labels=['out_range_low', 'potable','high']\n",
    "# categorize(water,'Potability',bins,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.116017Z",
     "start_time": "2021-07-17T14:19:55.277Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize(df, column, b, x_min, x_max):\n",
    "    potable = df[df['Potability']==1][column]\n",
    "    no_potable = df[df['Potability']==0][column]\n",
    "\n",
    "    y1, x1, _ = plt.hist(potable,bins=b)\n",
    "    y2, x2, _ = plt.hist(no_potable,bins=b)\n",
    "\n",
    "    plt.clf\n",
    "    plt.hist(potable, bins=b, alpha=0.5, label='potable')\n",
    "    plt.hist(no_potable, bins=b, alpha=0.01, label='non-potable')\n",
    "\n",
    "    plt.axvline(x=x_min, c='b', ls='--', alpha=.5)\n",
    "    plt.axvline(x=x_max, c='b', ls='--', alpha=.5)\n",
    "    \n",
    "    if y1.max() > y2.max():\n",
    "        y_top = y1.max()\n",
    "    else: \n",
    "        y_top = y2.max()\n",
    "\n",
    "#     print(y_top)\n",
    "    plt.fill_betweenx([0, y_top + 10], x_min, x_max, hatch=\"///\", facecolor=\"none\", edgecolor=\"b\", linewidth=0.01, alpha=.4)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. pH value:**\n",
    "PH is an important parameter in evaluating the acid–base balance of water. It is also the indicator of acidic or alkaline condition of water status. WHO has recommended maximum permissible limit of pH from 6.5 to 8.5. The current investigation ranges were 6.52–6.83 which are in the range of WHO standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.118013Z",
     "start_time": "2021-07-17T14:19:55.282Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 6.5, 8.5, 14, np.inf],\n",
    "    labels = ['out_range_low', 'acid', 'potable','basic','out_range_high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','ph'+'_cat']).agg(['count','min','max'])\n",
    "ranges.ph\n",
    "\n",
    "# water_ranges_by_potabillity.ph.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.119010Z",
     "start_time": "2021-07-17T14:19:55.287Z"
    }
   },
   "outputs": [],
   "source": [
    "# ranges = water_cat.groupby(['Potability','ph'+'_cat']).agg(['count','min','max'])\n",
    "# ranges.ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pH of gastric acid is 1.5 to 3.5. We have 0.22 classified as potable. Something is wrong. The classification of potability has 258 rows classified as potable with pH < 6.5 what is not considered potable by WHO.\n",
    "Let's take a look in the distribution of pH for the potable water in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.121004Z",
     "start_time": "2021-07-17T14:19:55.291Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'ph',30,6.5,8.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.122001Z",
     "start_time": "2021-07-17T14:19:55.295Z"
    }
   },
   "outputs": [],
   "source": [
    "potable = water[water['Potability']==1]\n",
    "potable[potable['ph'] < 6.5].ph.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Hardness:**\n",
    "Hardness is mainly caused by calcium and magnesium salts. These salts are dissolved from geologic deposits through which water travels. The length of time water is in contact with hardness producing material helps determine how much hardness there is in raw water. Hardness was originally defined as the capacity of water to precipitate soap caused by Calcium and Magnesium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.123995Z",
     "start_time": "2021-07-17T14:19:55.300Z"
    }
   },
   "outputs": [],
   "source": [
    "# c = pd.cut(\n",
    "#     water.stack(),\n",
    "#     [-np.inf, 0, 6.5, 8.5, 14, np.inf],\n",
    "#     labels = ['out_range_low', 'acid', 'potable','basic','out_range_high']\n",
    "# )\n",
    "# water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "# ranges = water_cat.groupby(['Potability','Hardness'+'_cat']).agg(['count','min','max'])\n",
    "# ranges.Hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.124994Z",
     "start_time": "2021-07-17T14:19:55.304Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Hardness',30,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Solids (Total dissolved solids - TDS):**\n",
    "Water has the ability to dissolve a wide range of inorganic and some organic minerals or salts such as potassium, calcium, sodium, bicarbonates, chlorides, magnesium, sulfates etc. These minerals produced un-wanted taste and diluted color in appearance of water. This is the important parameter for the use of water. The water with high TDS value indicates that water is highly mineralized. Desirable limit for TDS is 500 mg/l and maximum limit is 1000 mg/l which prescribed for drinking purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.125992Z",
     "start_time": "2021-07-17T14:19:55.310Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 500, 1000, np.inf],\n",
    "    labels=['out_range_low', 'low', 'potable','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Solids'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Solids\n",
    "# ranges[ranges.count > 0].Solids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.128093Z",
     "start_time": "2021-07-17T14:19:55.314Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Solids',30,500,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Chloramines:**\n",
    "Chlorine and chloramine are the major disinfectants used in public water systems. Chloramines are most commonly formed when ammonia is added to chlorine to treat drinking water. Chlorine levels up to 4 milligrams per liter (mg/L or 4 parts per million (ppm)) are considered safe in drinking water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.128982Z",
     "start_time": "2021-07-17T14:19:55.321Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 4, np.inf],\n",
    "    labels=['out_range_low', 'potable','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Chloramines'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Chloramines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.129981Z",
     "start_time": "2021-07-17T14:19:55.324Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Chloramines',30,0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Sulfate:**\n",
    "Sulfates are naturally occurring substances that are found in minerals, soil, and rocks. They are present in ambient air, groundwater, plants, and food. The principal commercial use of sulfate is in the chemical industry. Sulfate concentration in seawater is about 2,700 milligrams per liter (mg/L). It ranges from 3 to 30 mg/L in most freshwater supplies, although much higher concentrations (1000 mg/L) are found in some geographic locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.130979Z",
     "start_time": "2021-07-17T14:19:55.330Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 3, 30, np.inf],\n",
    "    labels=['out_range_low', 'low', 'potable','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Sulfate'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Sulfate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.131996Z",
     "start_time": "2021-07-17T14:19:55.334Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Sulfate',30,3,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Conductivity:**\n",
    "Pure water is not a good conductor of electric current rather’s a good insulator. Increase in ions concentration enhances the electrical conductivity of water. Generally, the amount of dissolved solids in water determines the electrical conductivity. Electrical conductivity (EC) actually measures the ionic process of a solution that enables it to transmit current. According to WHO standards, EC value should not exceeded 400 μS/cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.133970Z",
     "start_time": "2021-07-17T14:19:55.340Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 400, np.inf],\n",
    "    labels=['out_range_low', 'potable','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Conductivity'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Conductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.134986Z",
     "start_time": "2021-07-17T14:19:55.344Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Conductivity',30,0,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Organic_carbon:**\n",
    "Total Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. TOC is a measure of the total amount of carbon in organic compounds in pure water. According to US EPA < 2 mg/L as TOC in treated / drinking water, and < 4 mg/Lit in source water which is use for treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.136961Z",
     "start_time": "2021-07-17T14:19:55.353Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 2, 4, np.inf],\n",
    "    labels=['out_range_low', 'potable','to be treated','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Organic_carbon'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Organic_carbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.137959Z",
     "start_time": "2021-07-17T14:19:55.357Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Organic_carbon',30,0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Trihalomethanes:**\n",
    "THMs are chemicals which may be found in water treated with chlorine. The concentration of THMs in drinking water varies according to the level of organic material in the water, the amount of chlorine required to treat the water, and the temperature of the water that is being treated. THM levels up to 80 ppm is considered safe in drinking water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.138956Z",
     "start_time": "2021-07-17T14:19:55.363Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 80, np.inf],\n",
    "    labels=['out_range_low', 'potable','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Trihalomethanes'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Trihalomethanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.139953Z",
     "start_time": "2021-07-17T14:19:55.367Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Trihalomethanes',30,0,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Turbidity:**\n",
    "The turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of light emitting properties of water and the test is used to indicate the quality of waste discharge with respect to colloidal matter. The mean turbidity value obtained for Wondo Genet Campus (0.98 NTU) is lower than the WHO recommended value of 5.00 NTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.140950Z",
     "start_time": "2021-07-17T14:19:55.374Z"
    }
   },
   "outputs": [],
   "source": [
    "c = pd.cut(\n",
    "    water.stack(),\n",
    "    [-np.inf, 0, 5, np.inf],\n",
    "    labels=['out_range_low', 'potable','high']\n",
    ")\n",
    "water_cat = water.join(c.unstack().add_suffix('_cat'))\n",
    "ranges = water_cat.groupby(['Potability','Turbidity'+'_cat']).agg(['count','min','max'])\n",
    "ranges.Turbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.141947Z",
     "start_time": "2021-07-17T14:19:55.378Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize(water,'Turbidity',30,0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Principle components analysis [(PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)<a id='2.1_PCA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic steps in this process are:\n",
    "\n",
    "1. scale the data\n",
    "2. fit the PCA transformation (learn the transformation from the data)\n",
    "3. apply the transformation to the data to create the derived features\n",
    "4. (optionally) use the derived features to look for patterns in the data and explore the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Scale the data<a id='2.1.1_Scale_the_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.142957Z",
     "start_time": "2021-07-17T14:19:55.387Z"
    }
   },
   "outputs": [],
   "source": [
    "#New dataframe, `db_summary_scale` from `db_summary` whilst setting the index to '<selected var>'\n",
    "df_summary_scale = water_summary.set_index('ph',drop=True)\n",
    "water_summary = water_summary.set_index('ph',drop=True)\n",
    "#Save the index's labels (using the index attribute of `db_summary_scale`) into the variable 'db_summary_index'\n",
    "df_summary_index = df_summary_scale.index\n",
    "#Save the column names (using the `columns` attribute) of `db_summary_scale` into the variable 'db_summary_columns'\n",
    "df_summary_columns = df_summary_scale.columns\n",
    "df_summary_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.143943Z",
     "start_time": "2021-07-17T14:19:55.391Z"
    }
   },
   "outputs": [],
   "source": [
    "df_summary_scale.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Columns are all numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.144940Z",
     "start_time": "2021-07-17T14:19:55.395Z"
    }
   },
   "outputs": [],
   "source": [
    "df_summary_scale = scale(df_summary_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaling removes the column headers\n",
    "- Rename the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.145951Z",
     "start_time": "2021-07-17T14:19:55.401Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a new dataframe from `db_summary_scale` using the column names `db_summary_columns`\n",
    "df_summary_scaled_df = pd.DataFrame(df_summary_scale, columns=df_summary_columns)\n",
    "df_summary_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.1 Verifying the scaling<a id='2.1.1.1_Verifying_the_scaling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the scaling:\n",
    "- Check the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.146934Z",
     "start_time": "2021-07-17T14:19:55.407Z"
    }
   },
   "outputs": [],
   "source": [
    "#Call `db_summary_scaled_df`'s `mean()` method\n",
    "df_summary_scaled_df.agg(['mean','std']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- average is zero\n",
    "- all variables with the same standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking unbiased standard deviation estimator (ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.147932Z",
     "start_time": "2021-07-17T14:19:55.413Z"
    }
   },
   "outputs": [],
   "source": [
    "#Repeat the previous call to `std()` but pass in ddof=0 \n",
    "df_summary_scaled_df.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STD = 1 as expected in a scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculate the PCA transformation<a id='2.2_PCA_transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the PCA transformation using the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.149926Z",
     "start_time": "2021-07-17T14:19:55.419Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pca = PCA().fit(df_summary_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative variance ratio with number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.150924Z",
     "start_time": "2021-07-17T14:19:55.424Z"
    }
   },
   "outputs": [],
   "source": [
    "#Call the `cumsum()` method on the 'explained_variance_ratio_' attribute of `db_pca` and\n",
    "#create a line plot to visualize the cumulative explained variance ratio with number of components\n",
    "#Set the xlabel to 'Component #', the ylabel to 'Cumulative ratio variance', and the\n",
    "#title to 'Cumulative variance ratio explained by PCA components for state/resort summary statistics'\n",
    "#Hint: remember the handy ';' at the end of the last plot call to suppress that untidy output\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(df_pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel('Component #')\n",
    "plt.ylabel('Cumulative ratio variance')\n",
    "plt.title('Cumulative variance ratio explained by PCA components for ph summary statistics');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are no relevant components, all of them have the same influence in the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformation to the data to obtain the derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.151921Z",
     "start_time": "2021-07-17T14:19:55.429Z"
    }
   },
   "outputs": [],
   "source": [
    "#Call `db_pca`'s `transform()` method, passing in `db_summary_scale` as its argument\n",
    "df_pca_x = df_pca.transform(df_summary_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.153916Z",
     "start_time": "2021-07-17T14:19:55.433Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pca_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first two derived features (the first two principle components) and label each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.154914Z",
     "start_time": "2021-07-17T14:19:55.438Z"
    }
   },
   "outputs": [],
   "source": [
    "x = df_pca_x[:, 0]\n",
    "y = df_pca_x[:, 1]\n",
    "df_index = df_summary_index\n",
    "pc_var = 100 * df_pca.explained_variance_ratio_.cumsum()[1]\n",
    "plt.subplots(figsize=(10,8))\n",
    "plt.scatter(x=x, y=y)\n",
    "plt.xlabel('First component')\n",
    "plt.ylabel('Second component')\n",
    "plt.title(f'Water potability summary PCA, {pc_var:.1f}% variance explained')\n",
    "# for s, x, y in zip(water, x, y):\n",
    "#     plt.annotate(s, (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret this graph? (ASK Preet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Potability by ph<a id='2.3_Average_target_by_descriptor'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all point markers for the states are the same size and colour. You've visualized relationships between the states based on features such as the total skiable terrain area, but your ultimate interest lies in ticket prices. You know ticket prices for resorts in each state, so it might be interesting to see if there's any pattern there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.156907Z",
     "start_time": "2021-07-17T14:19:55.444Z"
    }
   },
   "outputs": [],
   "source": [
    "water.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.158903Z",
     "start_time": "2021-07-17T14:19:55.448Z"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate the average 'Potability' by state\n",
    "water_avg_potability = water.groupby('ph')['Potability'].mean()\n",
    "water_avg_potability.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.159900Z",
     "start_time": "2021-07-17T14:19:55.452Z"
    }
   },
   "outputs": [],
   "source": [
    "water_avg_potability.sort_values().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.160898Z",
     "start_time": "2021-07-17T14:19:55.456Z"
    }
   },
   "outputs": [],
   "source": [
    "water_avg_potability.hist(bins=30)\n",
    "plt.title('Distribution of potability')\n",
    "plt.xlabel('Potability')\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Adding potability to scatter plot<a id='2.4_Adding_average_target_to_scatter_plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.162893Z",
     "start_time": "2021-07-17T14:19:55.460Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a dataframe containing the values of the first two PCA components\n",
    "#Remember the first component was given by db_pca_x[:, 0],\n",
    "#and the second by db_pca_x[:, 1]\n",
    "#Call these 'PC1' and 'PC2', respectively and set the dataframe index to `db_summary_index`\n",
    "pca_df = pd.DataFrame({'PC1': df_pca_x[:, 0], 'PC2': df_pca_x[:, 1]}, index=df_summary_index)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.163889Z",
     "start_time": "2021-07-17T14:19:55.464Z"
    }
   },
   "outputs": [],
   "source": [
    "# our average state prices also have state as an index\n",
    "water_avg_potability.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.164887Z",
     "start_time": "2021-07-17T14:19:55.468Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can also cast it to a dataframe using Series' to_frame() method:\n",
    "water_avg_potability.to_frame().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating both parts on axis 1 and using the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.166889Z",
     "start_time": "2021-07-17T14:19:55.473Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate `pca_df` and `db_avg_price` along axis 1\n",
    "# remember, pd.concat will align on index\n",
    "pca_df = pd.concat([pca_df, water_avg_potability], axis=1)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.167878Z",
     "start_time": "2021-07-17T14:19:55.477Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.169874Z",
     "start_time": "2021-07-17T14:19:55.482Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df[pca_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.171869Z",
     "start_time": "2021-07-17T14:19:55.487Z"
    }
   },
   "outputs": [],
   "source": [
    "x = pca_df.PC1\n",
    "y = pca_df.PC2\n",
    "potability = pca_df.Potability\n",
    "ph = pca_df.index\n",
    "pc_var = 100 * df_pca.explained_variance_ratio_.cumsum()[1]\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for p in [0, 1]:\n",
    "    im = potability == p\n",
    "    ax.scatter(x=x[im], y=y[im], label=p)\n",
    "ax.set_xlabel('First component')\n",
    "ax.set_ylabel('Second component')\n",
    "plt.legend()\n",
    "ax.set_title(f'Water potability summary PCA, {pc_var:.1f}% variance explained')\n",
    "# for s, x, y in zip(ph, x, y):\n",
    "#     plt.annotate(s, (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding the potability to the scatterplot didn't reveal any clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much more concise way to do the same above using seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.172865Z",
     "start_time": "2021-07-17T14:19:55.492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a seaborn scatterplot by calling `sns.scatterplot`\n",
    "# Specify the dataframe pca_df as the source of the data,\n",
    "# specify 'PC1' for x and 'PC2' for y,\n",
    "# specify 'target' for the pointsize (scatterplot's `size` argument),\n",
    "# specify 'catergories' foar `hue`\n",
    "# specify sorter for `hue_order`\n",
    "x = pca_df.PC1\n",
    "y = pca_df.PC2\n",
    "ph = pca_df.index\n",
    "plt.subplots(figsize=(12, 10))\n",
    "# Note the argument below to make sure we get the colours in the ascending\n",
    "# order we intuitively expect!\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Potability', data=pca_df, hue_order=pca_df.Potability.sort_values(ascending=True).unique())\n",
    "# sns.scatterplot(x='PC1', y='PC2', size='Potability', hue='Quartile', \n",
    "#                 hue_order=pca_df.Quartile.cat.categories, data=pca_df)\n",
    "#if the amount of data allows, annotate with the labels\n",
    "# for s, x, y in zip(ph, x, y):\n",
    "#     plt.annotate(s, (x, y))   \n",
    "plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.173863Z",
     "start_time": "2021-07-17T14:19:55.496Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df_pca.components_, columns=df_summary_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No expressive values could be found in the components of the PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Numeric Data<a id='2.5.1_Numeric_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1.1 Feature engineering<a id='3.5.5.1_Feature_engineering'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.174870Z",
     "start_time": "2021-07-17T14:19:55.503Z"
    }
   },
   "outputs": [],
   "source": [
    "water_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No obvious derived metric can be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.5.2 Feature correlation heatmap<a id='3.5.5.2_Feature_correlation_heatmap'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great way to gain a high level view of relationships amongst the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.176856Z",
     "start_time": "2021-07-17T14:19:55.509Z"
    }
   },
   "outputs": [],
   "source": [
    "#Show a seaborn heatmap of correlations in data\n",
    "#Hint: call pandas' `corr()` method on `data` and pass that into `sns.heatmap`\n",
    "plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(water.corr(), annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No strog correlations could be found in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.5.3 Scatterplots of numeric features against Potability<a id='3.5.5.3_Scatterplots_of_numeric_features_against_ticket_price'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.177852Z",
     "start_time": "2021-07-17T14:19:55.514Z"
    }
   },
   "outputs": [],
   "source": [
    "# define useful function to create scatterplots of ticket prices against desired columns\n",
    "def scatterplots(columns, ncol=None, figsize=(15, 8)):\n",
    "    if ncol is None:\n",
    "        ncol = len(columns)\n",
    "    nrow = int(np.ceil(len(columns) / ncol))\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=figsize, squeeze=False)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.6)\n",
    "    for i, col in enumerate(columns):\n",
    "        ax = axes.flatten()[i]\n",
    "        ax.scatter(x = col, y = 'Potability', data=water, alpha=0.5)\n",
    "        ax.set(xlabel=col, ylabel='Potability')\n",
    "    nsubplots = nrow * ncol    \n",
    "    for empty in range(i+1, nsubplots):\n",
    "        axes.flatten()[empty].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.178849Z",
     "start_time": "2021-07-17T14:19:55.518Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use a list comprehension to build a list of features from the columns of `ski_data` that\n",
    "#are _not_ any of 'Name', 'Region', 'state', or 'AdultWeekend'\n",
    "# features = [f for f in water.columns if f not in ['Name', 'Region', 'state', 'AdultWeekend']]\n",
    "features = [f for f in water.columns if f not in ['Potability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.180845Z",
     "start_time": "2021-07-17T14:19:55.522Z"
    }
   },
   "outputs": [],
   "source": [
    "scatterplots(features, ncol=4, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The scatterplots don't show any significant split in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Summary<a id='3.6_Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we couldn't find classifiers to define the water as potable.\n",
    "\n",
    "- Numerical: 10\n",
    "\n",
    "- Categorical: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.182914Z",
     "start_time": "2021-07-17T14:19:55.529Z"
    }
   },
   "outputs": [],
   "source": [
    "water.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Contents<a id='4.1_Contents'></a>\n",
    "* [4 Pre-Processing and Training Data](#4_Pre-Processing_and_Training_Data)\n",
    "  * [4.1 Contents](#4.1_Contents)\n",
    "  * [4.2 Introduction](#4.2_Introduction)\n",
    "  * [4.3 Imports](#4.3_Imports)\n",
    "  * [4.4 Load Data](#4.4_Load_Data)\n",
    "  * [4.5 Extract Big Mountain Data](#4.5_Extract_Big_Mountain_Data)\n",
    "  * [4.6 Train/Test Split](#4.6_Train/Test_Split)\n",
    "  * [4.7 Initial Not-Even-A-Model](#4.7_Initial_Not-Even-A-Model)\n",
    "    * [4.7.1 Metrics](#4.7.1_Metrics)\n",
    "      * [4.7.1.1 R-squared, or coefficient of determination](#4.7.1.1_R-squared,_or_coefficient_of_determination)\n",
    "      * [4.7.1.2 Mean Absolute Error](#4.7.1.2_Mean_Absolute_Error)\n",
    "      * [4.7.1.3 Mean Squared Error](#4.7.1.3_Mean_Squared_Error)\n",
    "    * [4.7.2 sklearn metrics](#4.7.2_sklearn_metrics)\n",
    "        * [4.7.2.0.1 R-squared](#4.7.2.0.1_R-squared)\n",
    "        * [4.7.2.0.2 Mean absolute error](#4.7.2.0.2_Mean_absolute_error)\n",
    "        * [4.7.2.0.3 Mean squared error](#4.7.2.0.3_Mean_squared_error)\n",
    "    * [4.7.3 Note On Calculating Metrics](#4.7.3_Note_On_Calculating_Metrics)\n",
    "  * [4.8 Initial Models](#4.8_Initial_Models)\n",
    "    * [4.8.1 Imputing missing feature (predictor) values](#4.8.1_Imputing_missing_feature_(predictor)_values)\n",
    "      * [4.8.1.1 Impute missing values with median](#4.8.1.1_Impute_missing_values_with_median)\n",
    "        * [4.8.1.1.1 Learn the values to impute from the train set](#4.8.1.1.1_Learn_the_values_to_impute_from_the_train_set)\n",
    "        * [4.8.1.1.2 Apply the imputation to both train and test splits](#4.8.1.1.2_Apply_the_imputation_to_both_train_and_test_splits)\n",
    "        * [4.8.1.1.3 Scale the data](#4.8.1.1.3_Scale_the_data)\n",
    "        * [4.8.1.1.4 Train the model on the train split](#4.8.1.1.4_Train_the_model_on_the_train_split)\n",
    "        * [4.8.1.1.5 Make predictions using the model on both train and test splits](#4.8.1.1.5_Make_predictions_using_the_model_on_both_train_and_test_splits)\n",
    "        * [4.8.1.1.6 Assess model performance](#4.8.1.1.6_Assess_model_performance)\n",
    "      * [4.8.1.2 Impute missing values with the mean](#4.8.1.2_Impute_missing_values_with_the_mean)\n",
    "        * [4.8.1.2.1 Learn the values to impute from the train set](#4.8.1.2.1_Learn_the_values_to_impute_from_the_train_set)\n",
    "        * [4.8.1.2.2 Apply the imputation to both train and test splits](#4.8.1.2.2_Apply_the_imputation_to_both_train_and_test_splits)\n",
    "        * [4.8.1.2.3 Scale the data](#4.8.1.2.3_Scale_the_data)\n",
    "        * [4.8.1.2.4 Train the model on the train split](#4.8.1.2.4_Train_the_model_on_the_train_split)\n",
    "        * [4.8.1.2.5 Make predictions using the model on both train and test splits](#4.8.1.2.5_Make_predictions_using_the_model_on_both_train_and_test_splits)\n",
    "        * [4.8.1.2.6 Assess model performance](#4.8.1.2.6_Assess_model_performance)\n",
    "    * [4.8.2 Pipelines](#4.8.2_Pipelines)\n",
    "      * [4.8.2.1 Define the pipeline](#4.8.2.1_Define_the_pipeline)\n",
    "      * [4.8.2.2 Fit the pipeline](#4.8.2.2_Fit_the_pipeline)\n",
    "      * [4.8.2.3 Make predictions on the train and test sets](#4.8.2.3_Make_predictions_on_the_train_and_test_sets)\n",
    "      * [4.8.2.4 Assess performance](#4.8.2.4_Assess_performance)\n",
    "  * [4.9 Refining The Linear Model](#4.9_Refining_The_Linear_Model)\n",
    "    * [4.9.1 Define the pipeline](#4.9.1_Define_the_pipeline)\n",
    "    * [4.9.2 Fit the pipeline](#4.9.2_Fit_the_pipeline)\n",
    "    * [4.9.3 Assess performance on the train and test set](#4.9.3_Assess_performance_on_the_train_and_test_set)\n",
    "    * [4.9.4 Define a new pipeline to select a different number of features](#4.9.4_Define_a_new_pipeline_to_select_a_different_number_of_features)\n",
    "    * [4.9.5 Fit the pipeline](#4.9.5_Fit_the_pipeline)\n",
    "    * [4.9.6 Assess performance on train and test data](#4.9.6_Assess_performance_on_train_and_test_data)\n",
    "    * [4.9.7 Assessing performance using cross-validation](#4.9.7_Assessing_performance_using_cross-validation)\n",
    "    * [4.9.8 Hyperparameter search using GridSearchCV](#4.9.8_Hyperparameter_search_using_GridSearchCV)\n",
    "  * [4.10 Random Forest Model](#4.10_Random_Forest_Model)\n",
    "    * [4.10.1 Define the pipeline](#4.10.1_Define_the_pipeline)\n",
    "    * [4.10.2 Fit and assess performance using cross-validation](#4.10.2_Fit_and_assess_performance_using_cross-validation)\n",
    "    * [4.10.3 Hyperparameter search using GridSearchCV](#4.10.3_Hyperparameter_search_using_GridSearchCV)\n",
    "  * [4.11 Final Model Selection](#4.11_Final_Model_Selection)\n",
    "    * [4.11.1 Linear regression model performance](#4.11.1_Linear_regression_model_performance)\n",
    "    * [4.11.2 Random forest regression model performance](#4.11.2_Random_forest_regression_model_performance)\n",
    "    * [4.11.3 Conclusion](#4.11.3_Conclusion)\n",
    "  * [4.12 Data quantity assessment](#4.12_Data_quantity_assessment)\n",
    "  * [4.13 Save best model object from pipeline](#4.13_Save_best_model_object_from_pipeline)\n",
    "  * [4.14 Summary](#4.14_Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In preceding notebooks, performed preliminary assessments of data quality and refined the question to be answered. You found a small number of data values that gave clear choices about whether to replace values or drop a whole row. You determined that predicting the adult weekend ticket price was your primary aim. You threw away records with missing price data, but not before making the most of the other available data to look for any patterns between the states. You didn't see any and decided to treat all states equally; the state label didn't seem to be particularly useful.\n",
    "\n",
    "In this notebook you'll start to build machine learning models. Before even starting with learning a machine learning model, however, start by considering how useful the mean value is as a predictor. This is more than just a pedagogical device. You never want to go to stakeholders with a machine learning model only to have the CEO point out that it performs worse than just guessing the average! Your first model is a baseline performance comparitor for any subsequent model. You then build up the process of efficiently and robustly creating and assessing models against it. The development we lay out may be little slower than in the real world, but this step of the capstone is definitely more than just instructional. It is good practice to build up an understanding that the machine learning pipelines you build work as expected. You can validate steps with your own functions for checking expected equivalence between, say, pandas and sklearn implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Train/Test Split<a id='4.6_Train/Test_Split'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've treated ski resort data as a single entity. In machine learning, when you train your model on all of your data, you end up with no data set aside to evaluate model performance. You could keep making more and more complex models that fit the data better and better and not realise you were overfitting to that one set of samples. By partitioning the data into training and testing splits, without letting a model (or missing-value imputation) learn anything about the test split, you have a somewhat independent assessment of how your model might perform in the future. An often overlooked subtlety here is that people all too frequently use the test set to assess model performance _and then compare multiple models to pick the best_. This means their overall model selection process is  fitting to one specific data set, now the test split. You could keep going, trying to get better and better performance on that one data set, but that's  where cross-validation becomes especially useful. While training models, a test split is very useful as a final check on expected future performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What partition sizes would you have with a 70/30 train/test split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.184833Z",
     "start_time": "2021-07-17T14:19:55.537Z"
    }
   },
   "outputs": [],
   "source": [
    "water.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.185831Z",
     "start_time": "2021-07-17T14:19:55.541Z"
    }
   },
   "outputs": [],
   "source": [
    "# test drop missing\n",
    "water = water.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.186829Z",
     "start_time": "2021-07-17T14:19:55.545Z"
    }
   },
   "outputs": [],
   "source": [
    "water.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.187825Z",
     "start_time": "2021-07-17T14:19:55.549Z"
    }
   },
   "outputs": [],
   "source": [
    "len(water) * .7, len(water) * .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.189819Z",
     "start_time": "2021-07-17T14:19:55.553Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.190818Z",
     "start_time": "2021-07-17T14:19:55.558Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(water.drop(columns='Potability'), \n",
    "                                                    water.Potability, test_size=0.3, \n",
    "                                                    random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.191814Z",
     "start_time": "2021-07-17T14:19:55.562Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.192817Z",
     "start_time": "2021-07-17T14:19:55.565Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.194806Z",
     "start_time": "2021-07-17T14:19:55.569Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 1#\n",
    "#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test\n",
    "#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'\n",
    "# names_list = ['Name', 'state', 'Region']\n",
    "# names_train = X_train[names_list]\n",
    "# names_test = X_test[names_list]\n",
    "# X_train.drop(columns=names_list, inplace=True)\n",
    "# X_test.drop(columns=names_list, inplace=True)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.195804Z",
     "start_time": "2021-07-17T14:19:55.573Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 2#\n",
    "#Check the `dtypes` attribute of `X_train` to verify all features are numeric\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.196802Z",
     "start_time": "2021-07-17T14:19:55.577Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 3#\n",
    "#Repeat this check for the test split in `X_test`\n",
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have only numeric features in your X now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Initial Not-Even-A-Model<a id='4.7_Initial_Not-Even-A-Model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good place to start is to see how good the mean is as a predictor. In other words, what if you simply say your best guess is the average price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.197809Z",
     "start_time": "2021-07-17T14:19:55.582Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 4#\n",
    "#Calculate the mean of `y_train`\n",
    "train_mean = y_train.mean()\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s `DummyRegressor` easily does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.199793Z",
     "start_time": "2021-07-17T14:19:55.588Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 5#\n",
    "#Fit the dummy regressor on the training data\n",
    "#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments\n",
    "#Then print the object's `constant_` attribute and verify it's the same as the mean above\n",
    "dumb_reg = DummyRegressor(strategy='mean')\n",
    "dumb_reg.fit(X_train, y_train)\n",
    "dumb_reg.constant_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this? How closely does this match, or explain, the actual values? There are many ways of assessing how good one set of values agrees with another, which brings us to the subject of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Initial Models<a id='4.8_Initial_Models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.1 Imputing missing feature (predictor) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall when performing EDA, you imputed (filled in) some missing values in pandas. You did this judiciously for exploratory/visualization purposes. You left many missing values in the data. You can impute missing values using scikit-learn, but note that you should learn values to impute from a train split and apply that to the test split to then assess how well your imputation worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.1.1 Impute missing values with median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's missing values. Recall from your data exploration that many distributions were skewed. Your first thought might be to impute missing values using the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.1 Learn the values to impute from the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.200799Z",
     "start_time": "2021-07-17T14:19:55.597Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the values we'll use to fill in any missing values\n",
    "X_defaults_median = X_train.median()\n",
    "X_defaults_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.2 Apply the imputation to both train and test splits<a id='4.8.1.1.2_Apply_the_imputation_to_both_train_and_test_splits'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.202785Z",
     "start_time": "2021-07-17T14:19:55.601Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 9#\n",
    "#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use\n",
    "#Assign the results to `X_tr` and `X_te`, respectively\n",
    "X_tr = X_train.fillna(X_defaults_median)\n",
    "X_te = X_test.fillna(X_defaults_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.1.1.3 Scale the data<a id='4.8.1.1.3_Scale_the_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have features measured in many different units, with numbers that vary by orders of magnitude, start off by scaling them to put them all on a consistent scale. The [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) scales each feature to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.203783Z",
     "start_time": "2021-07-17T14:19:55.607Z"
    }
   },
   "outputs": [],
   "source": [
    "#Code task 10#\n",
    "#Call the StandardScaler`s fit method on `X_tr` to fit the scaler\n",
    "#then use it's `transform()` method to apply the scaling to both the train and test split\n",
    "#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr_scaled = scaler.transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.204779Z",
     "start_time": "2021-07-17T14:19:55.611Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/sohommajumder21/7-models-with-params-tuning-beginner-friendly/notebook\n",
    "#Hyperparameter tuning ;)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "xgb = XGBClassifier(eval_metric = 'logloss', use_label_encoder=False)\n",
    "\n",
    "\n",
    "para_knn = {'n_neighbors':np.arange(1, 50)}  #parameters of knn\n",
    "grid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5) #search knn for 5 fold cross validation\n",
    "\n",
    "#parameters for decision tree\n",
    "para_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\n",
    "grid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5) #grid search decision tree for 5 fold cv\n",
    "#\"gini\" for the Gini impurity and “entropy” for the information gain.\n",
    "#min_samples_leaf: The minimum number of samples required to be at a leaf node, have the effect of smoothing the model\n",
    "\n",
    "#parameters for random forest\n",
    "#n_estimators: The number of trees in the forest.\n",
    "params_rf = {'n_estimators':[100,200, 350, 500], 'min_samples_leaf':[2, 10, 30]}\n",
    "grid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)\n",
    "\n",
    "#parameters fpr AdaBoost\n",
    "params_ada = {'n_estimators': [50,100,250,400,500,600], 'learning_rate': [0.2,0.5,0.8,1]}\n",
    "grid_ada = GridSearchCV(ada, param_grid=params_ada, cv=5)\n",
    "\n",
    "#XGBoost\n",
    "#parameters for xgboost\n",
    "params_xgb = {'n_estimators': [50,100,250,400,600,800,1000], 'learning_rate': [0.2,0.5,0.8,1]}\n",
    "rs_xgb = RandomizedSearchCV(xgb, param_distributions=params_xgb, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.205777Z",
     "start_time": "2021-07-17T14:19:55.615Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_knn.fit(X_train, y_train)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "grid_ada.fit(X_train, y_train)\n",
    "rs_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for KNN:\", grid_knn.best_params_)\n",
    "print(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\n",
    "print(\"Best parameters for Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Best parameters for AdaBoost:\", grid_ada.best_params_)\n",
    "print(\"Best parameters for XGBoost:\", rs_xgb.best_params_)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=42, min_samples_leaf=1, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_leaf=2, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators= 600, learning_rate= 1 )\n",
    "xgb = XGBClassifier(n_estimators= 250, learning_rate= 0.8)\n",
    "\n",
    "#let's also apply bagging and boosting\n",
    "bagging = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=46, min_samples_leaf=2, random_state=42),\n",
    "                           n_estimators = 100, random_state = 42)\n",
    "bagging.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.206775Z",
     "start_time": "2021-07-17T14:19:55.619Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn),\n",
    "               ('Decision Tree', dt), ('Random Forest', rf), ('AdaBoost', ada),\n",
    "              ('Bagging Classifier', bagging), ('XGBoost', xgb)]\n",
    "\n",
    "for classifier_name, classifier in classifiers:\n",
    " \n",
    "    # Fit clf to the training set\n",
    "    classifier.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "\n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.2f}'.format(classifier_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.208769Z",
     "start_time": "2021-07-17T14:19:55.622Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rf= rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.210763Z",
     "start_time": "2021-07-17T14:19:55.626Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_knn.fit(X_tr, y_train)\n",
    "grid_dt.fit(X_tr, y_train)\n",
    "grid_rf.fit(X_tr, y_train)\n",
    "grid_ada.fit(X_tr, y_train)\n",
    "rs_xgb.fit(X_tr, y_train)\n",
    "\n",
    "print(\"Best parameters for KNN:\", grid_knn.best_params_)\n",
    "print(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\n",
    "print(\"Best parameters for Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Best parameters for AdaBoost:\", grid_ada.best_params_)\n",
    "print(\"Best parameters for XGBoost:\", rs_xgb.best_params_)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=42, min_samples_leaf=1, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_leaf=2, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators= 600, learning_rate= 1 )\n",
    "xgb = XGBClassifier(n_estimators= 250, learning_rate= 0.8)\n",
    "\n",
    "#let's also apply bagging and boosting\n",
    "bagging = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=46, min_samples_leaf=2, random_state=42),\n",
    "                           n_estimators = 100, random_state = 42)\n",
    "bagging.fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.211761Z",
     "start_time": "2021-07-17T14:19:55.630Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn),\n",
    "               ('Decision Tree', dt), ('Random Forest', rf), ('AdaBoost', ada),\n",
    "              ('Bagging Classifier', bagging), ('XGBoost', xgb)]\n",
    "\n",
    "for classifier_name, classifier in classifiers:\n",
    " \n",
    "    # Fit clf to the training set\n",
    "    classifier.fit(X_tr, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = classifier.predict(X_te)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "\n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.2f}'.format(classifier_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.213755Z",
     "start_time": "2021-07-17T14:19:55.635Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rf= rf.predict(X_te)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.214756Z",
     "start_time": "2021-07-17T14:19:55.639Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_knn.fit(X_tr_scaled, y_train)\n",
    "grid_dt.fit(X_tr_scaled, y_train)\n",
    "grid_rf.fit(X_tr_scaled, y_train)\n",
    "grid_ada.fit(X_tr_scaled, y_train)\n",
    "rs_xgb.fit(X_tr_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters for KNN:\", grid_knn.best_params_)\n",
    "print(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\n",
    "print(\"Best parameters for Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Best parameters for AdaBoost:\", grid_ada.best_params_)\n",
    "print(\"Best parameters for XGBoost:\", rs_xgb.best_params_)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=42, min_samples_leaf=1, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_leaf=2, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators= 600, learning_rate= 1 )\n",
    "xgb = XGBClassifier(n_estimators= 250, learning_rate= 0.8)\n",
    "\n",
    "#let's also apply bagging and boosting\n",
    "bagging = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=46, min_samples_leaf=2, random_state=42),\n",
    "                           n_estimators = 100, random_state = 42)\n",
    "bagging.fit(X_tr_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T14:19:58.215763Z",
     "start_time": "2021-07-17T14:19:55.644Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn),\n",
    "               ('Decision Tree', dt), ('Random Forest', rf), ('AdaBoost', ada),\n",
    "              ('Bagging Classifier', bagging), ('XGBoost', xgb)]\n",
    "\n",
    "for classifier_name, classifier in classifiers:\n",
    " \n",
    "    # Fit clf to the training set\n",
    "    classifier.fit(X_tr_scaled, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = classifier.predict(X_te_scaled)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "\n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.2f}'.format(classifier_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.1.2 Impute missing values with the mean<a id='4.8.1.2_Impute_missing_values_with_the_mean'></a>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
